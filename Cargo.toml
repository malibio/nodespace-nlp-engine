[package]
name = "nodespace-nlp-engine"
version = "0.1.0"
edition = "2021"
description = "AI/ML processing and SurrealDB integration for NodeSpace"
license = "MIT"
repository = "https://github.com/malibio/nodespace-nlp-engine"

[dependencies]
# Core NodeSpace types
nodespace-core-types = { path = "../nodespace-core-types" }

# Async runtime
tokio = { version = "1.0", features = ["full"] }
async-trait = "0.1"

# AI/ML and model inference (unified Candle stack)
candle-core = { version = "0.9.1", features = ["metal"], optional = true }
candle-nn = { version = "0.9.1", optional = true }
candle-transformers = { version = "0.9.1", optional = true }
hf-hub = { version = "0.4.2", features = ["tokio"], optional = true }
safetensors = { version = "0.4", optional = true }

# Optional LLM backend alternative
llama-cpp-2 = { version = "0.1.107", optional = true }

# Tokenization and text processing
tokenizers = { version = "0.19", optional = true }

# Serialization and data handling
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"

# Error handling
thiserror = "1.0"
anyhow = "1.0"

# Utilities
uuid = { version = "1.0", features = ["v4", "serde"] }
chrono = { version = "0.4", features = ["serde"] }
tracing = "0.1"
regex = "1.0"

# Performance and caching
dashmap = "5.0"
lru = "0.12"

[dev-dependencies]
tokio-test = "0.4"
tempfile = "3.0"
tracing-subscriber = "0.3"

[features]
default = ["real-ml"]
real-ml = ["candle-core", "candle-nn", "candle-transformers", "tokenizers", "hf-hub", "safetensors"]
llama-cpp = ["llama-cpp-2", "real-ml"]
cpu-only = ["real-ml"]

[[example]]
name = "generate_embeddings"
path = "examples/generate_embeddings.rs"

[[example]]
name = "text_generation"
path = "examples/text_generation.rs"

[lib]
name = "nodespace_nlp_engine"
path = "src/lib.rs"