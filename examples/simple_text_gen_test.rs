//! Simple test to isolate text generation model loading

use nodespace_nlp_engine::{LocalNLPEngine, NLPEngine};
use std::path::PathBuf;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Initialize tracing subscriber with simpler output
    tracing_subscriber::fmt()
        .with_max_level(tracing::Level::INFO)
        .with_target(true)
        .init();

    println!("ğŸ” Testing text generation model loading...");

    // Create engine
    let model_dir = PathBuf::from("../models");
    let engine = LocalNLPEngine::with_model_directory(model_dir);

    println!("ğŸ“¡ Initializing NLP engine...");
    match engine.initialize().await {
        Ok(()) => {
            println!("âœ… NLP engine initialized successfully");
        }
        Err(e) => {
            println!("âŒ NLP engine initialization failed: {}", e);
            return Err(e.into());
        }
    }

    println!("ğŸ¯ Testing simple text generation...");

    // Test with a very simple prompt to isolate issues
    let prompt = "Hello";
    println!("ğŸ“ Prompt: '{}'", prompt);

    let start = std::time::Instant::now();
    match engine.generate_text(prompt).await {
        Ok(response) => {
            let duration = start.elapsed();
            println!("â±ï¸  Duration: {:?}", duration);
            println!("ğŸ¤– Response: '{}'", response);

            // Very specific analysis
            if response == "A good team meeting requires clear objectives, structured agenda, active participation from all members, and defined action items." {
                println!("âŒ DETECTED: Canned meeting response - using stub implementation");
            } else if response == "Based on the available information, this requires further analysis to provide a comprehensive response." {
                println!("âŒ DETECTED: Generic canned response - using stub implementation");
            } else if response.contains("ONNX Runtime working") || response.contains("Generated response") {
                println!("âœ… DETECTED: ONNX inference response");
            } else {
                println!("ğŸ¤” UNKNOWN: Response '{}' - could be real ONNX", response);
            }
        }
        Err(e) => {
            println!("âŒ Text generation failed: {}", e);
            return Err(e.into());
        }
    }

    Ok(())
}
