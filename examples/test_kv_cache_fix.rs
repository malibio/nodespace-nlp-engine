//! Test the KV cache fix for ONNX inference
//!
//! This example tests the new KV cache implementation to ensure
//! all 54 required inputs are provided to the Gemma-3-1B ONNX model.

use nodespace_nlp_engine::{LocalNLPEngine, NLPEngine};
use std::path::PathBuf;
use tracing_subscriber;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Initialize comprehensive logging
    tracing_subscriber::fmt::init();

    println!("ğŸš€ Testing KV Cache Fix for ONNX Inference");
    println!("==========================================");
    println!("");

    // Create NLP engine with explicit model path
    let model_dir = PathBuf::from("/Users/malibio/nodespace/models");
    println!("ğŸ” Using model directory: {}", model_dir.display());

    let nlp_engine = LocalNLPEngine::with_model_directory(model_dir);
    println!("âœ… LocalNLPEngine created successfully");

    println!("ğŸ”§ Initializing NLP engine components...");
    if let Err(e) = nlp_engine.initialize().await {
        println!("âŒ Failed to initialize LocalNLPEngine: {}", e);
        return Err(e.into());
    }
    println!("âœ… LocalNLPEngine initialized successfully");

    println!("");
    println!("ğŸ”§ Testing simple text generation with KV cache fix...");

    // Test with a simple prompt
    let test_prompt = "Hello, how are you today?";
    println!("ğŸ”¤ Test prompt: '{}'", test_prompt);

    match nlp_engine.generate_text(test_prompt).await {
        Ok(response) => {
            println!("âœ… Text generation succeeded!");
            println!("ğŸ“ Response: {}", response);
            println!("ğŸ“ Response length: {} characters", response.len());

            // Check if this is still a fallback response
            if response.contains("fallback") || response.contains("technical difficulties") {
                println!("âš ï¸ WARNING: This appears to be a fallback response");
                println!("ğŸ” This indicates ONNX inference still failed despite KV cache fix");
                return Err("ONNX inference still failing".into());
            } else {
                println!("ğŸ‰ SUCCESS: This appears to be a real LLM-generated response!");
                println!("ğŸ‰ KV cache fix worked - ONNX inference is now functioning!");
            }
        }
        Err(e) => {
            println!("âŒ Text generation failed: {}", e);
            println!("ğŸ” Error details: {:?}", e);
            return Err(e.into());
        }
    }

    println!("");
    println!("ğŸ KV cache fix test completed successfully!");

    Ok(())
}
