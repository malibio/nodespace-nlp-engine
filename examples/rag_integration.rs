//! RAG Integration Example
//!
//! Demonstrates how to use the NLP engine for RAG (Retrieval-Augmented Generation) operations.
//! This example shows the pattern that nodespace-core-logic will use for orchestrating
//! semantic search + AI generation workflows.

use nodespace_nlp_engine::{
    LocalNLPEngine, NLPEngine, RAGContext, TextGenerationRequest,
    TokenBudget, estimate_token_count, allocate_budget_to_segments
};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Initialize tracing for better debugging
    tracing_subscriber::fmt::init();

    println!("ğŸš€ NodeSpace RAG Integration Example");
    println!("=====================================\n");

    // Step 1: Initialize NLP Engine
    println!("1ï¸âƒ£ Initializing NLP Engine...");
    let nlp_engine = LocalNLPEngine::new();
    nlp_engine.initialize().await?;
    println!("   âœ… NLP Engine initialized\n");

    // Step 2: Simulate retrieved knowledge from data store
    println!("2ï¸âƒ£ Simulating Knowledge Retrieval...");
    let retrieved_knowledge = vec![
        ("NodeSpace is a distributed system for knowledge management with AI integration.", 0.95),
        ("The system uses SurrealDB for data storage and local LLMs for AI processing.", 0.88),
        ("RAG (Retrieval-Augmented Generation) combines search with AI generation.", 0.82),
    ];
    
    for (text, score) in &retrieved_knowledge {
        println!("   ğŸ“„ Knowledge (score: {:.2}): {}", score, text);
    }
    println!();

    // Step 3: Token Budget Management
    println!("3ï¸âƒ£ Token Budget Management...");
    let context_window = 8192; // Typical model context window
    let mut budget = TokenBudget::new(context_window);
    
    // Estimate tokens for retrieved knowledge
    let knowledge_texts: Vec<String> = retrieved_knowledge.iter()
        .map(|(text, _)| text.to_string())
        .collect();
    
    let total_knowledge_tokens: usize = knowledge_texts.iter()
        .map(|text| estimate_token_count(text))
        .sum();
    
    budget.set_knowledge_tokens(total_knowledge_tokens);
    
    println!("   ğŸ“Š Context window: {} tokens", context_window);
    println!("   ğŸ“Š Knowledge content: {} tokens", total_knowledge_tokens);
    println!("   ğŸ“Š Available for context: {} tokens", budget.available_for_context());
    println!("   ğŸ“Š Within budget: {}\n", budget.is_within_budget());

    // Step 4: Smart content allocation if over budget
    println!("4ï¸âƒ£ Smart Content Allocation...");
    let max_knowledge_budget = budget.available_for_context() / 2; // Reserve half for conversation
    
    let prioritized_segments: Vec<(String, f32)> = retrieved_knowledge.iter()
        .map(|(text, score)| (text.to_string(), *score))
        .collect();
    
    let allocated_content = allocate_budget_to_segments(
        prioritized_segments,
        max_knowledge_budget
    )?;
    
    println!("   ğŸ¯ Allocated {} knowledge segments within budget", allocated_content.len());
    for (i, content) in allocated_content.iter().enumerate() {
        println!("   ğŸ“ Segment {}: {} chars", i + 1, content.len());
    }
    println!();

    // Step 5: Assemble RAG Context
    println!("5ï¸âƒ£ Assembling RAG Context...");
    let rag_context = RAGContext {
        knowledge_sources: allocated_content,
        retrieval_confidence: 0.88,
        context_summary: "NodeSpace distributed knowledge management system with AI capabilities".to_string(),
    };
    
    println!("   ğŸ§  Context summary: {}", rag_context.context_summary);
    println!("   ğŸ¯ Retrieval confidence: {:.2}", rag_context.retrieval_confidence);
    println!("   ğŸ“š Knowledge sources: {}\n", rag_context.knowledge_sources.len());

    // Step 6: Enhanced Text Generation with RAG
    println!("6ï¸âƒ£ Enhanced RAG Generation...");
    let user_query = "How does NodeSpace use AI for knowledge management?";
    
    let request = TextGenerationRequest {
        prompt: user_query.to_string(),
        max_tokens: 200,
        temperature: 0.7,
        context_window,
        conversation_mode: true,
        rag_context: Some(rag_context),
    };
    
    println!("   â“ User query: {}", user_query);
    println!("   âš™ï¸ Generation params: {} max tokens, {:.1} temperature", 
             request.max_tokens, request.temperature);
    
    let response = nlp_engine.generate_text_enhanced(request).await?;
    
    println!("\n   ğŸ¤– AI Response:");
    println!("   {}\n", response.text);
    
    // Step 7: Analysis and Metrics
    println!("7ï¸âƒ£ Response Analysis...");
    println!("   â±ï¸  Generation time: {}ms", response.generation_metrics.generation_time_ms);
    println!("   ğŸ“Š Context tokens: {}", response.generation_metrics.context_tokens);
    println!("   ğŸ“Š Response tokens: {}", response.generation_metrics.response_tokens);
    println!("   ğŸ¯ Context referenced: {}", response.context_utilization.context_referenced);
    println!("   ğŸ“š Sources mentioned: {:?}", response.context_utilization.sources_mentioned);
    println!("   ğŸ“ˆ Relevance score: {:.2}", response.context_utilization.relevance_score);

    println!("\nâœ¨ RAG Integration Example Complete!");
    println!("   This pattern can be used by nodespace-core-logic for:");
    println!("   â€¢ AIChatNode query processing");
    println!("   â€¢ Semantic search + generation workflows");  
    println!("   â€¢ Context-aware AI responses");
    println!("   â€¢ Token budget management");

    Ok(())
}